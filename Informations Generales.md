# limitations de la taille des prompts
Il n'y a pas de limite claire. La longueur courante d'un prompt est inf√©rieure √† 256 tokens, mais elle peut √™tre plus √©lev√©e, m√™me jusqu'√† 1024 tokens. (J'ai √©galement vu un post d'OpenAI indiquant 2000 mots maximum dans un prompt.) Les longs prompts peuvent √™tre difficiles √† comprendre pour le mod√®le et peuvent m√™me g√©n√©rer une r√©ponse inexacte. MAIS, lorsque vous pr√©sentez les donn√©es dans le prompt de mani√®re tr√®s structur√©e, vous r√©duisez √©galement le risque que le mod√®le ne comprenne pas ou g√©n√®re des r√©ponses ind√©sirables. R√®gle de base : "Moins, c'est mieux !"

## Tokens
Les mod√®les de la technologie GPT traitent les textes en utilisant des tokens, qui sont des s√©quences de caract√®res courantes dans les textes. Les mod√®les comprennent les relations statistiques entre ces tokens et excellent dans la production du token suivant dans une s√©quence de tokens. En √©tudiant la documentation de l'API GPT, je suis tomb√© sur un outil int√©ressant fourni par OpenAI appel√© Tokenizer (https://platform.openai.com/tokenizer). Vous pouvez utiliser cet outil pour comprendre comment un morceau de texte serait tokenis√© par l'API, et voir le nombre total de tokens dans ce morceau de texte. Il vous indique √©galement les identifiants des jetons.

**Ce qu'il faut noter:**
* De nombreux mots correspondent √† un seul jeton, mais d'autres non : indivisible.
* Les caract√®res Unicode tels que les emojis peuvent √™tre divis√©s en plusieurs jetons contenant les octets sous-jacents : ü§öüèæ
* Une r√®gle empirique utile est qu'un jeton correspond g√©n√©ralement √† environ 4 caract√®res de texte pour un texte anglais courant. Cela correspond √† environ ¬æ d'un mot (donc 100 tokens ~= 75 mots). Si nous prenons 1024 jetons comme maximum (sur la base des commentaires que j'ai trouv√©s dans certaines notes de l'API), cela se traduirait approximativement par 768 mots.

Nous avons d√©j√† vu des exemples o√π l'on utilise un prompt pour donner des instructions au mod√®le et o√π l'on soumet les donn√©es au prompt suivant pour traitement. Nous savons √©galement que le mod√®le "se souvient" de l'ensemble du fil de discussion. Cela m'am√®ne √† penser que nous pouvons diviser de grandes quantit√©s de donn√©es en blocs inf√©rieurs √† 1024 tokens / environ 768 mots. (L'une de mes prochaines exp√©riences consistera √† voir si / comment je peux faire en sorte que le mod√®le traite une transcription de plus de 5 000 mots et en extraie toutes les actions √† entreprendre).

